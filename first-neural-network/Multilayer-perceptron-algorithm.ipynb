{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description of the multilayer perceptron algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hypotheses : we consider a network with \n",
    "* I inputs with indices i = 1..I\n",
    "* H hidden neurons with indices h = 1..H. W\n",
    "* O output neurons with indice o = 1..O\n",
    "\n",
    "The perceptron weights and activation functions will be called \n",
    "* $W^H_{ih}$ for the weight between input i and hidden perceptron j, with activation function Hidden()\n",
    "* $W^O_{ho}$ for the weight between hidden j and ouput perceptrons o, with activation function Output()\n",
    "\n",
    "The vectors will be called \n",
    "* X for the input (vector of I elements $x_i$)\n",
    "* L for the output of the hidden layer (vector of H elements $l_h$)\n",
    "* Y for the desired output (vector of O elements $y_o$)\n",
    "* $\\hat{Y}$ for the output coming from the output neurons (vector of O elements $\\hat{y}_o$)\n",
    "\n",
    "*In what follows we will use Einstein's convention*\n",
    "\n",
    "The activation functions will be called $f^H()$ and $f^O()$. They are usually but not always the sigmoid function. \n",
    "A layer with an activation function f(x)=x is a regression layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an input vector $x_i$, we compute\n",
    "$ l_h = f^H(x_i * W^H_{ih}) $ and subsequently \n",
    "$ \\hat y_o = f^O(l_h * W^O_{ho}) $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce the following additional concepts :\n",
    "* the learning rate $ \\eta $\n",
    "* the error function $ E = \\frac{(y_o-\\hat y_o)^2}{2} $ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training for a single example :we have as training example the input vector X and desired output Y\n",
    "* Run a forward pass as mentioned above from $x_i$, leading to $l_h$ and $\\hat y_o$ \n",
    "* Backpropagation step 1 : compute the output error term for each output neuron $ \\delta^O_o = (y_o- \\hat y_o)* f'^O(l_h * W^O_{ho})  $\n",
    "* Backpropagation step 2 : compute the hidden error term for each hidden neuron $\\delta^H_h =  W^O_{ho} * \\delta^O_o * f'^H(x_i * W^H_{ih})  $\n",
    "* Update weights of the output neurons via $ \\Delta W^O_{ho} = \\eta * \\delta^O_o * l_h $\n",
    "* Update weights of the hidden neurons via $ \\Delta W^H_{ih} = \\eta * \\delta^H_h * x_i $\n",
    "\n",
    "*Remark : it is also possible to compute the $ \\Delta W^O_{ho} $ and $ \\Delta W^H_{ih}$ on several training samples and only update the weights at the end. Sometimes this is done with the whole training set at once.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
